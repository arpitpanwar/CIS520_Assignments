\section{Mind Refreshing SVM \hpoints{40}}

\begin{enumerate}
\item \textbf{Finite Features}: You are given the data set $D$ shown
  in in Figure~\ref{fig:svm_dataset} with data from a single feature
  $X_1$ in $\mathbb{R}^1$ and corresponding label $Y\in\{+,-\}$. The
  data set contains three positive examples at $X_1=\{-3,-2,3\}$ and
  three negative examples at $X_1=\{-1,0,1\}$.

  \begin{figure}[H]
    \begin{center}
      \includegraphics[width=4in]{images/svm_dataset.jpeg}
    \end{center}
    \caption{Dataset for SVM feature map task.}
    \label{fig:svm_dataset}
  \end{figure}
  
  This data set, \emph{in its current feature space}, cannot be
  perfectly separated using a linear separator.  The data changes
  class twice along only one dimension; a linear classifier in one
  dimension can only represent a single split.  In this problem, we'll
  investigate mapping the data to another feature space.

  \begin{enumerate}
  \item \points{2} Lets define the simple feature map
    $\phi(u)=(u,u^2)$ which transforms points in $\mathbb{R}^1$ to
    points in $\mathbb{R}^2$.  Apply $\phi$ to the data and plot the
    points in the new $\mathbb{R}^2$ feature space.
	
\begin{figure}[H]
    \begin{center}
      \includegraphics[width=4in]{images/1_1.jpg}
    \end{center}
    \caption{Transformed dataset for 1.b.}
    \label{fig:1.b}
  \end{figure}
  
  \item \points{2} Can a linear separator perfectly separate the
    points in the new $\mathbb{R}^2$ features space induced by $\phi$?
    If so, draw a linear separator that works.  If not, explain why
    not. \\
	Yes it can be linearly separated \\

\begin{figure}[h]
    \begin{center}
      \includegraphics[width=4in]{images/1_2.jpg}
    \end{center}
    \caption{ 1.c.}
    \label{fig:1.c}
  \end{figure}
   
  \item \points{4} Give the analytic form of the kernel that
    corresponds to the feature map $\phi$ in terms of only $X_1$ and
    $X_1^{\prime}$.  Specifically define $k(X_1,X_1^{\prime})$. 

\begin{align*}
	&\; \text{We have } \phi_1(u) = u \; and \; \phi_2(u) = u^2 \\
	 K(X_1,X_1^{\prime}) =& \sum\limits_{j=1}^2 \phi_j(x_1) \phi_j(x_1^{\prime})\\
	 =&\; \phi_1(x_1) \phi_1(x_1^{\prime}) + \phi_2(x_1) \phi_2(x_1^{\prime}) \\
	=&\; X_1 X_1^{\prime} + X_1^2 (X_1^{\prime})^2  \\
	=&\; X_1 X_1^{\prime} (1 + X_1 X_1^{\prime} ) \\
\end{align*}
  
  \item \points{7} Construct a maximum-margin separating hyperplane.
    This hyperplane will be a line in $\mathbb{R}^2$, which can be
    parameterized by its normal equation, i.e. $w_1 Y_1 + w_2 Y_2 + c
    = 0$ for appropriate choices of $w_1,w_2,c$.  Here, $(Y_1,Y_2) =
    \phi(X_1)$ is the result of applying the feature map $\phi$ to the
    original feature $X_1$. Give the values for $w_1, w_2, c$.  Also,
    explicitly compute the margin for your hyperplane.  You do not
    need to solve a quadratic program to find the maximum margin
    hyperplane.  Instead, let your geometric intuition guide you.

\begin{align*}
	& \text{Finding the midpoint of the points } (-1 ,1) \text{ and } (-2 ,4) \\
	=& (-1.5 , 2.5) \\
	&\text{Slope of the line is } \dfrac{y_2 - y_1}{x_2 - x_1} \\
	=& \frac{1}{-3} \\
	& \text{Line is }y = \frac{-1}{3} x +c \\
	& \text{Putting in value of midpoint and solving for c we get} \\
	c =& 3 \\
	3y - x - 9 =& 0\\
\end{align*}
  
  \item \points{4} On the plot of the transformed points (from part
    3), plot the separating hyperplane and the margin, and circle the
    support vectors.

\begin{figure}[H]
    \begin{center}
      \includegraphics[width=4in]{images/1_3.jpg}
    \end{center}
    \caption{Transformed dataset for 1.d.}
    \label{fig:1.b}
  \end{figure}

  \item \points{2} Draw the decision boundary separating of the
    separating hyperplane, in the original $\mathbb{R}^1$ feature
    space.

\begin{figure}[H]
    \begin{center}
      \includegraphics[width=4in]{images/1_4.jpeg}
    \end{center}
    \caption{Transformed dataset for 1.e.}
    \label{fig:1.b}
  \end{figure}
  
  \item \points{4} Compute the coefficients $\alpha$ and the constant
    $b$ in \autoref{eqn:svm_form} for the kernel $k$ and the support
    vectors $SV=\{u_1,u_2\}$ you chose in part 6.  Be sure to explain
    how you obtained these coefficients.
    \begin{eqnarray}
      y(x) = \sign \left( \sum_{i=1}^{|SV|} \alpha_i y_i k(x, u_i) + b \right)
      \label{eqn:svm_form}
    \end{eqnarray}
    Think about the dual form of the quadratic program and the
    constraints placed on the $\alpha$ values.
\begin{align*}
	&\text{We know that the dual of SVM is given by }\\
	& max_{\alpha \ge 0} \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j k(x_i,x_j) \\
	& \text{such that} \sum_i \alpha_i y_i \\
	& \text{Substituing values for alphas} \\
	& \alpha_1 (1) + \alpha_2 (-1) = 0 \\
	\alpha_1 =& \alpha_2 = \alpha \\
	&\text{Now }\\
	b =&\; y_i - \sum_j \alpha_j y_j k(x_j , x_i ) \text{ when } \alpha \ge 0 \\
	b =&\; 1 - \sum_j \alpha y_j k(x_j , -2) \\
	=&\; 1 -(\alpha k(-2,-2) - \alpha k(-1,-2)) \\
	=&\; 1 - 14\alpha \\
	&\text{ Again }\\
	b =&\; -1 - \sum_j \alpha y_j k(x_j , -1) \\
	=&\; -1 - (\alpha k(-2,-1) - \alpha k(-1,-1)) \\
	=&\; -1 -4\alpha \\
	& \text{From above we can say that} \\
	1 - 14 \alpha =& \; -1 -4 \alpha \\
	\alpha =&\;  \frac{1}{5} \\
	b = &\; 1 - 14 \frac{1}{5} \\
	=&\; \frac{-9}{5} \\
\end{align*}


  \item \points{2} If we add another positive ($Y=+$) point to the
    training set at $X_1=5$ would the hyperplane or margin change?
    Why or why not?
	The hyperplane will not change because the point lies beyond the margin (SVM Bound).

  \end{enumerate}
  
\item \textbf{Infinite Features Spaces and Kernel Magic}: Lets define
  a new (infinitely) more complicated feature transformation
  $\phi_n:\mathbb{R}^1 \rightarrow \mathbb{R}^n$ given in
  \autoref{eqn:phi}.
  \begin{eqnarray}
    \phi_n(x) = \left\{ e^{-x^2/2}, e^{-x^2/2} x, \frac{e^{-x^2/2}x^2}{\sqrt{2}},
    \ldots,
    \frac{e^{-x^2/2}x^i}{\sqrt{i!}}
    \ldots,
    \frac{e^{-x^2/2}x^n}{\sqrt{n!}}
    \right\}
    \label{eqn:phi}
  \end{eqnarray}
  Suppose we let $n \rightarrow \infty$ and define new feature
  transformation in \autoref{eqn:hilbert_vector}.  You can think of
  this feature transformation as taking some finite feature vector and
  producing an infinite dimensional feature vector rather than the
  simple two dimensional feature vector used in the earlier part of
  this problem.
  \begin{eqnarray}
    \phi_{\infty}(x) = \left\{ e^{-x^2/2}, e^{-x^2/2} x,
    \frac{e^{-x^2/2}x^2}{\sqrt{2}},
    \ldots,
    \frac{e^{-x^2/2}x^i}{\sqrt{i!}}
    \ldots
    \right\}
    \label{eqn:hilbert_vector}
  \end{eqnarray}
  
  \begin{enumerate}
  \item \points{3} Assuming a \emph{consistent} dataset, is there a
    finite set of points that cannot be linearly separated in this
    feature space?  If so, give an example dataset.  If not, explain
    why not.  Note: A consistent dataset is one where there aren't any
    points $\bx_i = \bx_j$ where $y_i \ne y_j$.  That is, no point can
    be labeled multiple different things. \\
  It is linearly separable as there are no points that map to the same set of features. \\
 We can see from the set of features that there can be no case where 2 different dataset values have the same values in every dimension.\\	

  \item \points{4} We know that we can express a linear classifier
    using only inner products of support vectors in the transformed
    feature space as seen in \autoref{eqn:svm_form}.  It would be
    great if we could some how use the feature space obtained by the
    feature transformation $\phi_{\infty}$.  However, to do this we
    must be able to compute the inner product of examples in this
    infinite vector space.  Lets define the inner product between two
    infinite vectors $a=\{a_1,\ldots,a_i,\ldots\}$ and
    $b=\{b_1,\ldots,b_i,\ldots\}$ as the infinite sum given in
    \autoref{eqn:innerproduct_sum}.
    \begin{eqnarray}
      k(a,b) = a \cdot b = \sum_{i=1}^{\infty} a_i b_i
      \label{eqn:innerproduct_sum}
    \end{eqnarray}
    We cannot explicitly compute $k(a,b)$ in this form since it
    contains an infinite sum.  However, we can re-write $k(a,b)$ in a
    form that is efficiently computable.  Derive the efficiently
    computable form.  \emph{Hint}: You may want to use the Taylor
    series expansion of $e^x$ which is given in
    \autoref{eqn:taylorseries}.
    \begin{eqnarray}
      e^{x}=\lim_{n \rightarrow \infty} \sum_{i=0}^n \frac{x^i}{i!}
      \label{eqn:taylorseries}
    \end{eqnarray}
\begin{align*}
	& \text{We can generalize the phi as } \phi_{j}(x) = \dfrac{e^{-\frac{x^2}{2}} x^{j-1}}
                                                                                                          {\sqrt{! (j-1)}} \\
	&\text{Substituting it in the equation for kernel }\\
	K(a,b) = & \; \sum\limits_{j=1}^{\infty} \phi_j(a) \phi_j(b) \\
	=& \sum\limits_{j=1}^{\infty} \dfrac{e^{-\frac{a^2}{2}} x^{j-1}}
                                                                                                          {\sqrt{! (j-1)}} \dfrac{e^{-\frac{b^2}{2}} x^{j-1}}
                                                                                                          {\sqrt{! (j-1)}} \\
	= & \sum\limits_{i=0}^{\infty} \dfrac{e^{-\frac{a^2}{2}} x^{i}}
                                                                                                          {\sqrt{! (i)}} \dfrac{e^{-\frac{b^2}{2}} x^{i}}
                                                                                                          {\sqrt{! (i)}} \\
	=& e^{\frac{-a^2 - b^2}{2}}   \sum \limits_{i=0}^{\infty} \dfrac{a^{i} b^{i}}
                                                                                                          {\sqrt{! (i)}}  \\
	& \text{Using taylor series} \\
	=& e^{\frac{-a^2 - b^2}{2}} e^{ab} \\
	&\;\text{Solving the powers} \\
	=& \; e^{-\frac{(a-b)^2}{2}} \\
\end{align*}

  \item \points{6}  Prove or disprove each claim below.
  
    \begin{enumerate}
    \item Suppose we translate the inputs $x'_i = x_i + x_0$ for some
      arbitrary $x_0$ before using the kernel above in an SVM.  Will my
      kernel function change? (i.e., does $k(x_i,x_j) = k(x'_i,x'_j)$)?
\begin{align*}
	&\text{We can see the kernel from above } e^{-\dfrac{(a-b)^2}{2}} \\
	&\text{Substituting the values for a and b} \\
	& e^{-\dfrac{( x_1 + x_0  - x_2 - x_0)^2}{2}} \\
	=&  e^{-\dfrac{( x_1 - x_2 )^2}{2}} \\
	\text{Thus noting changes}\\
\end{align*}
     
    \item What if we negated all the inputs $x'_i = -x_i$? \\
    \text{Similar to above the on substituting a with -a nothing changes as we take the square of the values which is same for positive and negative values } \\
    \item What about rescaling $x_i' = ax_i$ for some positive scalar $a$?
\begin{align*}
		&\text{Substituting the values for a and b } \\
		& e^{-\dfrac{( a x_1 - a x_2 )^2}{2}} \\
		=& \; e^{- a^2 \dfrac{( x_1 -  x_2 )^2}{2}} \\
		\text{As we can see rescaling changes the kernel} \\
\end{align*}
      
      
    \end{enumerate}  
  \end{enumerate}
\end{enumerate}
  
\PointStats
